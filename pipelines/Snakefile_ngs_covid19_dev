#https://snakemake.bitbucket.io/snakemake-tutorial.html
#@author James Boocock
#@date March 31, 2020

## We should change this input
configfile:  workflow.current_basedir + "/defaults.yaml" 
CONFIGFILE= workflow.current_basedir + "/defaults.yaml" 

include: "rules/config.smk"
include: "rules/utils.smk"

depth_range = [int(MIN_DEPTH)]
min_cov = 0.8

taxon_dict = read_taxid(TAXON_IN)
metadata=fastq_match(config)
#type_uid 
#metadata = metadata[metadata["sample"].isin([72,83,107,131,367])]
samples=metadata["sample"]
# uid type merges libraries across samples
try:
    if config["merge_rule"] == "uid_type": 
        mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_name_and_readgroup()
    # merges all libraries by uid
    elif config["merge_rule"] == "uid":
        #  print("HERE")
        mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_name() 
    elif config["merge_rule"] == "uid_oligo":
        mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_hash(merge_rule="uid_oligo") 
    elif config["merge_rule"] == "column_uid_sample_type":
        mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_hash(merge_rule="column_uid_sample_type") 

except KeyError:
    mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_name_and_readgroup()
    pass

 #   print(mapped_uid)

### metadata["sample"]
#### metadata["strain"]

#mapped_uid, sample_names_hash, mapped_uids_read_one, mapped_uids_read_two = uid_library_to_sample_name_and_readgroup()
#with open("sample_mapping.txt","w") as out_f:
#    for key, items in mapped_uid.items():
#        print(key)
#        print(items)
#        out_f.write(str(key) + " " + " ".join([str(o) for o in items]) + "\n")
#samples=["2011"]
#rule all:
#    input:
#        "outputs/qc_report/qc_report.tsv"


rule all:
    input:
        expand("outputs/consensus/merged/phased/{intervals}/all_{depth}_none.fasta", depth=depth_range, intervals=intervals),
        "outputs/quasi_species/merged/all_from_merged.vcf",
        "outputs/quasi_species/merged/phased.vcf.gz",
        expand("outputs/mapping_stats/bam_subset/merged/{uid_lib}.bam", uid_lib=mapped_uid.keys()),
        "outputs/amplicon_qc/umi/amplicon_df.txt",
        expand("outputs/mapping_stats/md/merged/{uid_lib}.bam", uid_lib=mapped_uid.keys()),
        #expand("outputs/kraken2/{sample}.kraken_report.txt",sample=samples),
        expand("outputs/consensus/merged/{intervals}/all_{depth}.fasta", depth=depth_range, intervals=intervals),
        expand("outputs/abundances/merged/{sample}.bracken", sample=mapped_uid.keys()), 
        expand("outputs/coverage/merged/coverage_plots/{intervals}/{sample}.png", intervals=intervals, sample=mapped_uid.keys()), 
        expand("outputs/mapping_stats/bam_subset/{sample}.bam", sample=samples),
        "outputs/ncov/merged/all.fasta",
        "outputs/bcftools_vcfs/merged/all.vcf.gz",
        "outputs/quasi_species/merged/all.vcf",
#        expand("outputs/raw_fastqs/{sample}.R1.fastq.gz",sample=samples),
       # TODO check if it works#
      #"test.png",
        expand("outputs/coverage/coverage_plots/{intervals}/{sample}.png", intervals=intervals, sample=samples), 
        #expand("outputs/kraken_bams/{intervals}/{sample}.kraken.bam",intervals=intervals,sample=samples),
        #expand("outputs/kraken_bams/{intervals}/{sample}.kraken.bam",sample=samples,intervals=intervals),
        #expand("outputs/vcfs_filt/{intervals}/all.filt.vcf.gz",intervals=intervals),
        #expand("outputs/consensus/{depth}/{intervals}/{sample}.fasta",depth=depth_range,sample=samples,intervals=intervals),
        #expand("outputs/coverage/{depth}/{intervals}/{sample}.cov", depth=depth_range, sample=samples, intervals=intervals),
        expand("outputs/coverage/{intervals}/{sample}.cov", sample=samples,intervals=intervals),
        #expand("outputs/insert_sizes/{intervals}/{sample}.insert_sizes",sample=samples,intervals=intervals),
        expand("outputs/consensus/{intervals}/all_{depth}.fasta", depth=depth_range, intervals=intervals),
        #expand("outputs/abundances/{sample}.bracken",sample=samples),
        #"{SCRIPTS_DIR}/quick_align.sh '{params.rg}' {SARS_REF} {input.read_one} {input.read_two} {output} {log}" 
        expand("outputs/consensus/bams/{intervals}/{sample}/{depth}.bam",intervals=intervals,sample=samples, depth=depth_range),
        expand("outputs/depleted_fastqs/human/{sample}.R1.fastq.gz",sample=samples),
       # bai="outputs/consensus/bams/{intervals}/{sample}/{depth}.bam.bai"
        #expand("outputs/igv-report/{intervals}/{sample}.html", intervals=intervals, sample=samples),
        # TODO: Reimplement the mapping from 
        #
        # TODO: Combinne all sample information we can gather from the sequencing into a single file. 
        "outputs/amplicon_qc/amplicon_df.txt",
        "outputs/amplicon_qc/amplicon_merged_df.txt",
        "outputs/final/abundances.bracken",
        "outputs/pangolin/lineages.csv",
        #"outputs/final/qc_report.tsv",
        "outputs/qc_report/qc_report.tsv",
        directory(expand("outputs/final/merged/coverage_plots/{intervals}/",intervals=intervals)),
        #""outputs/final/filtered/metadata.csv",
        "outputs/final/merged/qc_report.tsv",
        expand("outputs/bcftools_vcfs/merged/filt/{intervals}/all.vcf.gz",intervals=intervals),
        "outputs/final/merged/annovar/all_filtered_annovar_table.txt",
        "outputs/final/merged/quasi_species/all.vcf"
#rule all:
    #    input:
        #        expand("outputs/consensus/merged/phased/{intervals}/all_{depth}_none.fasta", depth=depth_range, intervals=intervals),
#        "outputs/pangolin/lineages_merged.csv"

#print(samples

mapped_sample_names = {}
for key, value in mapped_uid.items():
    for val in value:
        mapped_sample_names[str(val)] = key 

include: "rules/align.smk"
include: "rules/qc_metrics.smk"
include: "rules/variant_calling.smk"
include: "rules/phylogenetics.smk"
#
#

rule qc_input:
    input:
        qc_report_in="outputs/qc_report/all.qc",
    output:
        csv_per_library="outputs/qc_report/qc_report.tsv"
    run:
        all_qc = pd.read_csv(input.qc_report_in,sep=" ")
        print(all_qc["sample"])
        md_m= metadata.merge(all_qc,left_on="sample",right_on="sample")
        md_m = md_m.fillna(value="NA")
        print(metadata["sample"])
        md_m.to_csv(output.csv_per_library,sep="\t", index=False)  

rule generate_final_dataset_package:
    input:
        qc_report_in="outputs/qc_report/all.qc",
        pangolin_out="outputs/pangolin/lineages_merged.csv",
        input_consensus = expand("outputs/consensus/merged/{intervals}/all_{depth}.fasta", depth=MIN_DEPTH, intervals=intervals),
        input_consensus_phylo = expand("outputs/consensus/merged/phylo/{intervals}/all_{depth}.fasta", depth=MIN_DEPTH, intervals=intervals),
        read_one = expand("outputs/filtered_fastqs/{intervals}/{sample}.R1.fastq.gz",intervals=intervals,sample=samples),
        read_two = expand("outputs/filtered_fastqs/{intervals}/{sample}.R2.fastq.gz",intervals=intervals, sample=samples),
        depleted_read_one = expand("outputs/depleted_fastqs/human/{sample}.R1.fastq.gz",sample=samples),
        depleted_read_two = expand("outputs/depleted_fastqs/human/{sample}.R2.fastq.gz", sample=samples),
        merged_vcf = "outputs/bcftools_vcfs/merged/filt/sars2/all.vcf.gz"
    output:
        merged_csv=report("outputs/final/merged/qc_report.tsv",category="qc_reports/final",caption=workflow.basedir + "/report/coverage_plots.rst"),
        merged_csv_unfiltered=report("outputs/qc_report/merged/qc_report.tsv",category="qc_reports/merged",caption=workflow.basedir + "/report/coverage_plots.rst"),
        #report(directory("outputs/final/merged/coverage_plots/{intervals}/"),patterns=["{sample}.png"],category="coverage_plots/final_coverage_plots",caption=workflow.basedir+ "/report/coverage_plots.rst")
        filtered_fastqs=directory("outputs/final/merged/filtered_fastqs") ,
        depeleted_fastqs=directory("outputs/final/merged/host_depleted_fastqs"),
        consensus_fasta="outputs/final/merged/all.fasta",
        final_vcf="outputs/final/merged/all.vcf.gz",
        phylo_fasta="outputs/final/merged/phylo/all.fasta"
    run:
        all_qc = pd.read_csv(input.qc_report_in,sep=" ")
        # Let's create the QC table for the merged libraries.
        ## Temaplate for the pandas.
        row_tuples = []
        coverage_tuples =[]
        library_tuples = []
        for key, samples in mapped_uid.items():
            qc_rows = all_qc[all_qc["sample"].isin(samples)]
            tmp_row = {}
            qc_rows = qc_rows[["mapped_human","mapped_rrna","mapped_sars2_dedup","mapped_sars2_dup","unmapped","total_read_count"]].apply(np.sum,axis=0)
            #qc_rows = qc_rows[["mapped_human","mapped_rrna","mapped_sars2","unmapped","total_read_count"]].apply(np.sum,axis=0)
            metadata[metadata["sample"].isin(samples)]
            row_tuples.append((key, qc_rows["mapped_human"],qc_rows["mapped_rrna"], qc_rows["mapped_sars2_dedup"],qc_rows["mapped_sars2_dup"], qc_rows["unmapped"],qc_rows["total_read_count"] )) 
                ### now we load in everything we need to calculate i.e mapping stat ##
            with open("outputs/consensus/merged/sars2/{MIN_DEPTH}/{sample}.cov".format(sample=key,MIN_DEPTH=MIN_DEPTH)) as in_f:
                for line in in_f:
                    coverage_tuples.append((key, float(line), sample_names_hash[key][0],sample_names_hash[key][1]))
            cov_in = pd.read_csv("outputs/coverage/merged/sars2/{sample}.cov".format(sample=key),sep="\t",header=None) 
            mean_coverage_dedup = np.mean(cov_in[2])
            cov_in = pd.read_csv("outputs/coverage/merged/sars2/{sample}.no_dedup.cov".format(sample=key),sep="\t",header=None) 
            mean_coverage_no_dedup = np.mean(cov_in[2])
            coverage_5x = np.sum(cov_in[2] >= 5)/len(cov_in[2])
            tmp_metadata=metadata[metadata["sample"].isin(samples)]
            ### TODO: add to finnal column as josh requested
            lib_names =",".join(tmp_metadata["lib_name"]) 
            fastq_one =",".join(tmp_metadata["fastq_one"]) 
            fastq_two = ",".join(tmp_metadata["fastq_two"])
            samples_str = [str(sample) for sample in samples]
            samples_out = ",".join(samples_str)
            library_tuples.append((key, lib_names, fastq_one, fastq_two, samples_out ,mean_coverage_dedup, mean_coverage_no_dedup,coverage_5x))
        coverage_5x_df = pd.DataFrame(coverage_tuples, columns=["merged_id","coverage_3x","read_group_name","uid_lib"])
        library_df = pd.DataFrame(library_tuples, columns=["merged_id","lib_names","fastqs_one","fastqs_two","library_id_internal","coverage_mean_dedup","coverage_mean_no_dedup","coverage_5x"])
        out_df = pd.DataFrame(row_tuples, columns=["merged_id","mapped_human","mapped_rrna","mapped_sars2_dedup","mapped_sars2_dup","unmapped","total_read_count"])
        #sars2_percent_dedup","sars2_percent_dup"
        out_df["sars2_percent_dup"] = out_df["mapped_sars2_dup"]/out_df["total_read_count"]
        out_df["sars2_percent_dedup"] = out_df["mapped_sars2_dedup"]/out_df["total_read_count"]
        #out_df["sars2_percent"] = out_df["mapped_sars2"]/(out_df["mapped_human"] + out_df["mapped_rrna"] + out_df["mapped_sars2"]) * 100
        pango_in = pd.read_csv(input.pangolin_out, sep="\t")
        out_df = out_df.merge(coverage_5x_df, left_on="merged_id", right_on="merged_id", how="outer")
        out_df = out_df.merge(library_df, left_on="merged_id", right_on="merged_id", how="outer")
        out_df = out_df.merge(pango_in, left_on="read_group_name", right_on="taxon", how="outer")
        #### 
        ####
        #nextstrain_clade="outputs/clades/nextstrain/lineages.csv",
        #gisaid_clades="outputs/clades/gisaid/lineages.csv"
 
        ##nextstrain_in = pd.read_csv(input.nextstrain_clade,sep="\t",header=True,skiprows=1,names=["read_group_name","gisaid_clade","gisaid_parent_clade"]) 
        ### Rename columns to nextstrain.
        #
        #
            
        ## Now merged with metadata .... ### 
        out_df = out_df.merge(metadata,left_on="uid_lib",right_on="id_library_type")
        out_df = out_df.drop_duplicates(subset=["uid_lib"])
        out_df =  out_df.drop(columns=["sample_x","sample_y","strain","lib_name","fastq_one","fastq_two","run_name"])
        out_df=out_df.fillna(value="NA")
        sample_name_fasta_vcf = [key.replace("_","-") for key in mapped_uid.keys() ]
        out_df["sample_name_fasta"] = sample_name_fasta_vcf
        out_df.to_csv(output.merged_csv_unfiltered, sep="\t",index=False)
        # Drop irrelvente collum
        shell("touch {output.consensus_fasta}")
        shell("touch {output.merged_csv}")
        shell("touch {output.final_vcf}")
        shell("touch {output.phylo_fasta}")
        if os.path.getsize(input.input_consensus[0]) != 0:
            shutil.copy(input.input_consensus[0], output.consensus_fasta)
            shutil.copy(input.input_consensus_phylo[0], output.phylo_fasta)
            os.makedirs("outputs/final/merged/filtered_fastqs/",exist_ok=True)
            os.makedirs("outputs/final/merged/host_depleted_fastqs/",exist_ok=True)
            in_fasta = pyfasta.Fasta(input.input_consensus[0])
            kept_ids = [] 
            for key in in_fasta.keys():
                rows = metadata["sample"][metadata["id_library_type"].isin(out_df["uid_lib"][out_df["read_group_name"]  == key])]
                merged_id = str(out_df["merged_id"][out_df["read_group_name"]==key]).split()[1]
                kept_ids.append(merged_id)
            with open("tmp_sample_list.txt","w") as out_f: 
                for sample in kept_ids:
                    out_f.write(sample.replace("_","-")+ "\n")
            shell("bcftools view -S tmp_sample_list.txt {input.merged_vcf} | bgzip -c > {output.final_vcf} && tabix -p vcf {output.final_vcf}")
            os.remove("tmp_sample_list.txt")
            for key in in_fasta.keys():
                rows = metadata["sample"][metadata["id_library_type"].isin(out_df["uid_lib"][out_df["read_group_name"]  == key])]
                merged_id = str(out_df["merged_id"][out_df["read_group_name"]==key]).split()[1]
                for row in rows:
                    shell("cat outputs/filtered_fastqs/sars2/{sample}.R1.fastq.gz >> outputs/final/merged/filtered_fastqs/{key}.R1.fastq.gz".format(sample=row,key=merged_id))
                    shell("cat outputs/filtered_fastqs/sars2/{sample}.R1.fastq.gz >> outputs/final/merged/filtered_fastqs/{key}.R2.fastq.gz".format(sample=row,key=merged_id))
                    shell("cat outputs/depleted_fastqs/human/{sample}.R1.fastq.gz >> outputs/final/merged/host_depleted_fastqs/{key}.R1.fastq.gz".format(sample=row,key=merged_id))
                    shell("cat outputs/depleted_fastqs/human/{sample}.R1.fastq.gz >> outputs/final/merged/host_depleted_fastqs/{key}.R2.fastq.gz".format(sample=row,key=merged_id))
            
            out_df = out_df[out_df["merged_id"].isin(kept_ids)] 
            out_df.to_csv(output.merged_csv, sep="\t",index=False)

        else:
           print("No libraries passed QC")
#





#rule generate_filtered_data:
#    input:
#        input_consensus = expand("outputs/consensus/{intervals}/all_{depth}.fasta", depth=MIN_DEPTH, intervals=intervals),
#        read_one = expand("outputs/filtered_fastqs/{intervals}/{sample}.R1.fastq.gz",intervals=intervals,sample=samples),
#        read_two = expand("outputs/filtered_fastqs/{intervals}/{sample}.R2.fastq.gz",intervals=intervals, sample=samples),
#        qc_report_in = "outputs/final/qc_report.tsv" 
#    output:
#        complete=report("outputs/final/filtered/metadata.csv",
#        out_dir=directory("outputs/final/filtered/filtered_fastqs/")
#    run:
#        if os.path.getsize(input.input_consensus[0]) != 0:
#            in_fasta = pyfasta.Fasta(input.input_consensus[0])
#            genomes_passed_filter = in_fasta.keys()
#            # Replace the slashes
#            os.makedirs(output.out_dir,exist_ok=True)
#            qc_report_in = pd.read_csv(input.qc_report_in, sep="\t")
#            out_names = []
#            for genome in genomes_passed_filter:
#                genome_name="-".join(genome.split("/"))
#                out_names.append(genome_name)
#                number = int(genome_name.split("-")[3])
#                for read_one, read_two in zip(input.read_one, input.read_two):
#                    read_one_base = os.path.basename(read_one).split(".")[0]
#                    if read_one_base == genome_name:
#                        shutil.copy(read_one, os.path.join(output.out_dir,os.path.basename(read_one)))
#                        shutil.copy(read_two, os.path.join(output.out_dir,os.path.basename(read_two)))
#            subset = qc_report_in[qc_report_in["sample"].isin(out_names)]
#            ### Add pangolin clades. 
#            subset.to_csv(output.complete,index=False, sep="\t")
#        else:
#            print("No libraries passed QC")
#            shell("touch {output.complete}")
#

